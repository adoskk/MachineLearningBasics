{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "\n",
    "# all_docs = []\n",
    "# for docs in reader.iter_data():\n",
    "#     # <do something with the documents per file>\n",
    "#     all_docs.extend(docs)\n",
    "    \n",
    "# print(len(all_docs))\n",
    "# print(all_docs[0].id_)\n",
    "# print(all_docs[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "318\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "import glob\n",
    "\n",
    "pages = []\n",
    "for file_path in glob.glob(\"data/*.pdf\"):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages.extend(loader.load_and_split())\n",
    "    \n",
    "print(len(pages))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "db = FAISS.from_documents(chunks, embedding)\n",
    "db.save_local('faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\qwen\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is mixture of experts', 'result': ' The Mixture of Experts (MoE) refers to a type of neural network architecture where multiple \"expert\" networks work together to make predictions or decisions based on the input data. Each expert network specializes in solving different sub-problems, and a gating network determines which expert(s) should be used for a given input. This architecture allows for flexible and adaptive learning, as it can handle complex problems that may contain many sub-problems each requiring different experts. In this context, the MoE serves as a general-purpose neural network component.', 'source_documents': [Document(metadata={'source': 'data\\\\moe.pdf', 'page': 2}, page_content='the mixture-of-experts approach has been the subject of much research. Different types of expert\\narchitectures hae been proposed such as SVMs (Collobert et al., 2002), Gaussian Processes (Tresp,\\n2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009),\\nand deep networks. Other work has focused on different expert conﬁgurations such as a hierarchical\\nstructure (Yao et al., 2009), inﬁnite numbers of experts (Rasmussen & Ghahramani, 2002), and\\nadding experts sequentially (Aljundi et al., 2016). Garmash & Monz (2016) suggest an ensemble\\nmodel in the format of mixture of experts for machine translation. The gating network is trained on\\na pre-trained ensemble NMT model.\\nThe works above concern top-level mixtures of experts. The mixture of experts is the whole model.\\nEigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as\\nparts of a deep model. It is intuitive that the latter approach is more powerful, since complex prob-'), Document(metadata={'source': 'data\\\\moe.pdf', 'page': 13}, page_content='Under review as a conference paper at ICLR 2017\\nResults: Results are reported in Table 6. All the combinations containing at least one the two\\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\\nvalues ofwloadhad lower loads on the most overloaded expert.\\nB H IERACHICAL MIXTURE OF EXPERTS\\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\\nbination of “experts\", each of which is itself a secondary mixture-of-experts with its own gating\\nnetwork.3If the hierarchical MoE consists of agroups ofbexperts each, we denote the primary gat-\\ning network by Gprimary , the secondary gating networks by (G1,G2..Ga), and the expert networks\\nby(E0,0,E0,1..Ea,b). The output of the MoE is given by:\\nyH=a∑\\ni=1b∑\\nj=1Gprimary (x)i·Gi(x)j·Ei,j(x) (12)\\nOur metrics of expert utilization change to the following:\\nImportance H(X)i,j=∑'), Document(metadata={'source': 'data\\\\moe.pdf', 'page': 2}, page_content='mixture-of-experts with its own gating network. In the following we focus on ordinary MoEs. We\\nprovide more details on hierarchical MoEs in Appendix B.\\nOur implementation is related to other models of conditional computation. A MoE whose experts are\\nsimple weight matrices is similar to the parameterized weight matrix proposed in (Cho & Bengio,\\n2014). A MoE whose experts have one hidden layer is similar to the block-wise dropout described\\nin (Bengio et al., 2015), where the dropped-out layer is sandwiched between fully-activated layers.\\n3'), Document(metadata={'source': 'data\\\\moe.pdf', 'page': 2}, page_content='lems may contain many sub-problems each requiring different experts. They also allude in their\\nconclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational\\ncomputation.\\nOur work builds on this use of MoEs as a general purpose neural network component. While Eigen\\net al. (2013) uses two stacked MoEs allowing for two sets of gating decisions, our convolutional\\napplication of the MoE allows for different gating decisions at each position in the text. We also\\nrealize sparse gating and demonstrate its use as a practical way to massively increase model capacity.\\n2 T HESTRUCTURE OF THE MIXTURE -OF-EXPERTS LAYER\\nThe Mixture-of-Experts (MoE) layer consists of a set of n“expert networks\" E1,···,En, and a\\n“gating network\" Gwhose output is a sparse n-dimensional vector. Figure 1 shows an overview\\nof the MoE module. The experts are themselves neural networks, each with their own parameters.')]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "embedding = HuggingFaceEmbeddings()\n",
    "    \n",
    "model = Ollama(model='mistral')\n",
    "\n",
    "query = input(\"\\nQuery: \")\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the following question from the pdf files,\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "    \n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        model,\n",
    "        retriever=db.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mixture of Experts (MoE) is a hierarchical neural network model that is used to solve complex problems by delegating them to a set of specialized sub-networks called \"experts.\" The idea is to have multiple models, each specialized in handling different parts of the input space.\n",
      "\n",
      "In MoE, a gating network determines the weight of each expert based on the input and chooses the best or a combination of the experts to make predictions. This allows for more efficient and accurate modeling compared to having a single large model that attempts to handle all possible inputs directly. It is particularly useful in situations where data is diverse or complex, as it can adaptively allocate computational resources across the problem space.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the following question from the pdf files,\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "print(chain.invoke({\"question\": \"What is mixture of experts?\"}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
