{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2 Model PyTorch Implementation\n",
    "This is a notebook implementing a mini version of Llama2 model with only 6 transformer layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RMSNorm\n",
    "\n",
    "reference: \\\n",
    "https://github.com/meta-llama/llama/blob/8fac8befd776bc03242fe7bc2236cdb41b6c609c/llama/model.py#L34\\\n",
    "https://arxiv.org/abs/1910.07467"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class rmsnorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model):\n",
    "        self.eps = 1e-8\n",
    "        self.scale = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = (batch, seq_len, d_model)\n",
    "        _, _, d_model = x.size() \n",
    "        rms = torch.norm(x, dim=-1) / d_model\n",
    "        x = x / (rms + self.eps)\n",
    "        x = torch.einsum(\"ijk,k->ijk\", x, self.scale)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rotary Position Embedding\n",
    "reference:\\\n",
    "https://arxiv.org/abs/2104.09864\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotary_position(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super.__init__(self)\n",
    "        \n",
    "        self.theta = torch.zeros(d_model)\n",
    "        self.theta[::2] = (1/10000)**(2*torch.arange(d_model/2)/d_model)\n",
    "        self.theta[1::2] = (1/10000)**(2*torch.arange(d_model/2)/d_model)\n",
    "        \n",
    "        self.cos = torch.cos(torch.einsum(\"n,k->nk\", torch.arange(seq_len), self.theta))\n",
    "        self.sin = torch.sin(torch.einsum(\"n,k->nk\", torch.arange(seq_len), \n",
    "                                          torch.einsum(\"k,k->k\", (-1)**torch.arange(d_model), self.theta)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = torch.einsum(\"nk,bnk->bnk\", self.cos, x) +\\\n",
    "            torch.einsum(\"nk,bnk->bnk\", self.sin, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Group Query Attention\n",
    "References:\\\n",
    "https://github.com/fkodom/grouped-query-attention-pytorch/blob/main/grouped_query_attention_pytorch/attention.py#L203\\\n",
    "https://arxiv.org/abs/2305.13245v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class group_query_attention(nn.torch):\n",
    "    def __init__(self, seq_len, query_heads, kv_heads, d_model, mask=None):\n",
    "        \"\"\"\n",
    "        MHA:\n",
    "        \n",
    "        | (q, q, ..., q)  | (q, q, ..., q) | ... | (q, q, ..., q) | -> query_dim = num_heads * head_dim\n",
    "        | (k, k, ..., k)  | (k, k, ..., k) | ... | (k, k, ..., k) | ->   key_heads = num_heads * head_dim\n",
    "        | (v, v, ..., v)  | (v, v, ..., v) | ... | (v, v, ..., v) | -> value_heads = num_heads * head_dim\n",
    "        \n",
    "        =====================================================================\n",
    "        GQA:\n",
    "        \n",
    "        | (q, q, ..., q) (q, q, ..., q)...  |  ... | (q, q, ..., q) (q, q, ..., q)|    ->    query_dim = query_heads * head_dim\n",
    "        |       (k/v, k/v, ..., k/v)       |   ... |       (k/v, k/v, ..., k/v)      | ->    kv_dim = kv_heads * head_dim\n",
    "                                                                                             group_nums = query_heads / kv_heads\n",
    "        \n",
    "        \"\"\"\n",
    "        super.__init__(self)\n",
    "        \n",
    "        self.seq_len = seq_len\n",
    "        self.query_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "        self.d_model = d_model\n",
    "        self.group_nums = self.query_heads / self.kv_heads\n",
    "        self.head_dim = self.d_model / self.query_heads   \n",
    "    \n",
    "        self.WQ = nn.Linear(d_model, d_model)\n",
    "        self.WK = nn.Linear(d_model, self.head_dim * self.kv_heads)\n",
    "        self.WV = nn.Linear(d_model, self.head_dim * self.kv_heads)\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.rope = rotary_position(seq_len, d_model)    \n",
    "            \n",
    "    def split_group_head(self, x, group_nums, head_dim, type=\"q\"):\n",
    "        # input q, k, v: (batch, seq_len, x_dim)\n",
    "        # output q: (batch, group_nums, group_dim, seq_len, head_dim)\n",
    "        # output k: (batch, group_nums, seq_len, head_dim)\n",
    "        # output v: (batch, group_nums, seq_len, head_dim)\n",
    "        \n",
    "        batch, seq_len, x_dim = x.size()\n",
    "        if type == \"q\":\n",
    "            group_dim = int(x_dim / head_dim / group_nums)\n",
    "            x = x.view(batch, seq_len, group_nums, group_dim, head_dim)\n",
    "            x = torch.permute(x, [0, 2, 3, 1, 4])\n",
    "        else:\n",
    "            x = x.view(batch, seq_len, group_nums, head_dim).transpose(1, 2)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def concat_head(self, x):\n",
    "        # input x: (batch, num_heads, seq_len, d_model // num_heads)\n",
    "        # output x: (batch, seq_len, d_model)\n",
    "        batch, num_heads, seq_len, d_head = x.size()\n",
    "        x = x.transpose(1, 2).view(batch, seq_len, num_heads*d_head)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def group_dot_attention(self, q, k, v):\n",
    "        # q - (batch, group_nums, group_dim, seq_len, head_dim)\n",
    "        # k - (batch, group_nums, seq_len, head_dim)\n",
    "        # v - (batch, group_nums, seq_len, head_dim)\n",
    "        \n",
    "        attention = torch.einsum(\"bgdsh,bgsh->bgdsh\", q, k)\n",
    "        attention = self.softmax(attention/torch.sqrt(self.d_model))\n",
    "        output = torch.einsum(\"bgdsh,bgsh->bgdsh\", attention, v)\n",
    "        return output, attention\n",
    "        \n",
    "    def forward(self, q, k, v):\n",
    "        # q: (batch, seq_len, d_model)\n",
    "        # k: (batch, seq_len, kv_dim)\n",
    "        # v: (batch, seq_len, kv_dim)\n",
    "        \n",
    "        q = self.rope(self.WQ(q))\n",
    "        k = self.rope(self.WK(k))\n",
    "        v = self.WV(v)\n",
    "        \n",
    "        q = self.split_group_head(q, self.group_nums, self.head_dim, \"q\")\n",
    "        k = self.split_group_head(k, self.group_nums, self.head_dim, \"k\")\n",
    "        v = self.split_group_head(v, self.group_nums, self.head_dim, \"v\")\n",
    "        \n",
    "        # q - (batch, group_nums, group_dim, seq_len, head_dim)\n",
    "        # k - (batch, group_nums, seq_len, head_dim)\n",
    "        # v - (batch, group_nums, seq_len, head_dim)\n",
    "        \n",
    "        output, attention = self.group_dot_attention(q, k, v)\n",
    "        \n",
    "        output = self.concat_head(output)\n",
    "        \n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Llama model\n",
    "![Llama model architecture](images/llama_architecture.png)\n",
    "\n",
    "For simplicity, we set Nx=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer_layer(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super.__init__(self)\n",
    "        \n",
    "        self.rmsnorm = rmsnorm()\n",
    "        self.gqa = group_query_attention()\n",
    "        self.ffw = nn.Sequential([nn.Linear(d_model, 2*d_model),\n",
    "                                  nn.SiLU(),\n",
    "                                  nn.Linear(2*d_model, d_model),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.rmsnorm(x)\n",
    "        x, _ = self.gqa(x, x, x)\n",
    "        x = self.ffw(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "        \n",
    "class llama_model(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super.__init__(self)\n",
    "        \n",
    "        self.transformer_layers = nn.Sequential([\n",
    "            transformer_layer() for _ in range(6)\n",
    "        ])\n",
    "        self.rmsnorm = rmsnorm()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.transformer_layers(x)\n",
    "        x = self.rmsnorm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
